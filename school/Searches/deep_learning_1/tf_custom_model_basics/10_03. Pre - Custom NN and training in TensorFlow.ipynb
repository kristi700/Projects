{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jQbAP7UcV1ch"
   },
   "outputs": [],
   "source": [
    "#@title MIT License\n",
    "#\n",
    "# Copyright (c) 2020 Balázs Pintér\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a\n",
    "# copy of this software and associated documentation files (the \"Software\"),\n",
    "# to deal in the Software without restriction, including without limitation\n",
    "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
    "# and/or sell copies of the Software, and to permit persons to whom the\n",
    "# Software is furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
    "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
    "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
    "# DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vBipUPSaV1cl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 14:25:42.552615: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-19 14:25:42.553431: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 14:25:42.555812: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-19 14:25:42.562584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732026342.574394   38332 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732026342.577636   38332 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-19 14:25:42.590313: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gE-ixnlFV1cm"
   },
   "outputs": [],
   "source": [
    "num_neurons = 20\n",
    "num_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7PMFYifGXEOS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yvRgwB-xV1cm"
   },
   "outputs": [],
   "source": [
    "# getting the data, like last time\n",
    "\n",
    "corpus, targets = zip(*[(movie_reviews.raw(fileid), category)\n",
    "                         for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)])\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_df=0.95, min_df=2, max_features=num_features)\n",
    "bows = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# convert targets to numbers\n",
    "targets = np.array([0 if target == 'neg' else 1 for target in targets])\n",
    "\n",
    "bows = bows.astype(np.float32)\n",
    "targets = targets.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(bows, targets, test_size=0.1, shuffle=True)\n",
    "\n",
    "# the problem: we have sparse arrays, but neural network need dense arrays!\n",
    "# the solution will be word embeddings, here we just convert to dense arrays\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9M2xPv6DV1cm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 14:25:45.334280: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in TensorFlow 2.0, we can see the tensor instantly (same as eager mode in TensorFlow 1.0)\n",
    "tf.ones((4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rSVhN-zcV1cn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 7, 9], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we also see the results of an operation instantly\n",
    "tf.add(tf.constant([1, 2, 3]), tf.constant([4, 5, 6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z7zEid_IV1cn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       "array([[4., 4., 4., 4.],\n",
       "       [4., 4., 4., 4.],\n",
       "       [4., 4., 4., 4.],\n",
       "       [4., 4., 4., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros((4, 4)) + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Cc0YnPk4V1cn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'W1:0' shape=(20, 5000) dtype=float32, numpy=\n",
       " array([[ 0.00055755,  0.01734037, -0.01524723, ...,  0.00191359,\n",
       "         -0.00475368, -0.00106215],\n",
       "        [-0.00899945, -0.01513705,  0.00947089, ..., -0.00455601,\n",
       "          0.00875724,  0.00752879],\n",
       "        [ 0.00485277,  0.01038518, -0.00593169, ...,  0.01099314,\n",
       "          0.00319859, -0.01897568],\n",
       "        ...,\n",
       "        [-0.00926714,  0.00126028, -0.01369052, ...,  0.0137404 ,\n",
       "         -0.00135495, -0.00367781],\n",
       "        [-0.00779927,  0.00469138,  0.01839181, ..., -0.00223047,\n",
       "         -0.00155172,  0.01317024],\n",
       "        [ 0.00353124, -0.0007916 , -0.01210199, ...,  0.00124528,\n",
       "          0.0017817 ,  0.0059835 ]], dtype=float32)>,\n",
       " <tf.Variable 'b1:0' shape=(20, 1) dtype=float32, numpy=\n",
       " array([[-0.01236881],\n",
       "        [ 0.01129107],\n",
       "        [-0.00497256],\n",
       "        [-0.01576513],\n",
       "        [ 0.00188694],\n",
       "        [-0.01615887],\n",
       "        [ 0.00146614],\n",
       "        [-0.01671192],\n",
       "        [-0.01026485],\n",
       "        [ 0.00751555],\n",
       "        [-0.01177625],\n",
       "        [-0.04072687],\n",
       "        [ 0.01127723],\n",
       "        [ 0.00943432],\n",
       "        [-0.01344921],\n",
       "        [ 0.00265316],\n",
       "        [-0.007474  ],\n",
       "        [ 0.00330612],\n",
       "        [ 0.01498286],\n",
       "        [-0.01265052]], dtype=float32)>,\n",
       " <tf.Variable 'W2:0' shape=(1, 20) dtype=float32, numpy=\n",
       " array([[ 0.008535  ,  0.01922821, -0.00363132, -0.00132372,  0.00655555,\n",
       "         -0.01136575, -0.00063787, -0.00769452,  0.00619579,  0.00217565,\n",
       "         -0.00193463, -0.01488594,  0.01021304, -0.00398029, -0.00178151,\n",
       "         -0.00926346,  0.00585189, -0.00100828,  0.00017619, -0.01065083]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'b2:0' shape=(1, 1) dtype=float32, numpy=array([[-0.01084862]], dtype=float32)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variables are for the parameters (weights) for the NN\n",
    "W1 = tf.Variable(tf.random.normal((num_neurons, num_features), stddev=0.01), name='W1')\n",
    "b1 = tf.Variable(tf.random.normal((num_neurons, 1), stddev=0.01), name='b1')\n",
    "W15 = tf.Variable(tf.random.normal((num_neurons, num_neurons), stddev=0.01), name='W15')\n",
    "b15 = tf.Variable(tf.random.normal((num_neurons, 1), stddev=0.01), name='b15')\n",
    "W2 = tf.Variable(tf.random.normal((1, num_neurons), stddev=0.01), name='W2')\n",
    "b2 = tf.Variable(tf.random.normal((1, 1), stddev=0.01), name='b2')\n",
    "W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-yBqyuQrV1cn"
   },
   "outputs": [],
   "source": [
    "# the network itself\n",
    "def forward_pass(X):\n",
    "    z = tf.add(tf.matmul(W1, X), b1)\n",
    "    a = tf.nn.relu(z)\n",
    "    z = tf.add(tf.matmul(W15, a), b15)\n",
    "    a = tf.nn.relu(z)\n",
    "    z = tf.add(tf.matmul(W2, a), b2)\n",
    "    #a = tf.nn.sigmoid(z) # already in loss function\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ckEmVbBqV1co"
   },
   "outputs": [],
   "source": [
    "# the loss function\n",
    "def loss(a, y):\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=a, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8eWYbBvzV1co"
   },
   "outputs": [],
   "source": [
    "def minibatches(X, y, minibatch_size):\n",
    "\n",
    "# shuffling\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    X_shuffled = X[perm]\n",
    "    y_shuffled = y[perm]\n",
    "\n",
    "    mini_batches = [(X_shuffled[i*minibatch_size:(i+1)*minibatch_size], y_shuffled[i*minibatch_size:(i+1)*minibatch_size])\n",
    "                   for i in range(X_shuffled.shape[0] // minibatch_size)]\n",
    "# we lost some examples at the end, doesn't really matter for this example\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qKy4Pv4fV1cp"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwd3cUCyV1cp"
   },
   "source": [
    "$w_{11} := w_{11} - lr*\\frac{loss}{\\delta w_{11}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "pKFp7sfmV1cq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in epoch 0: 0.6937556266784668, accuracy: 0.5016666666666667\n",
      "Loss in epoch 1: 0.6942073702812195, accuracy: 0.5016666666666667\n",
      "Loss in epoch 2: 0.6941079497337341, accuracy: 0.5016666666666667\n",
      "Loss in epoch 3: 0.693834125995636, accuracy: 0.49833333333333335\n",
      "Loss in epoch 4: 0.6931800842285156, accuracy: 0.49833333333333335\n",
      "Loss in epoch 5: 0.6551679372787476, accuracy: 0.8144444444444444\n",
      "Loss in epoch 6: 0.490001916885376, accuracy: 0.8027777777777778\n",
      "Loss in epoch 7: 0.3423937261104584, accuracy: 0.9538888888888889\n",
      "Loss in epoch 8: 0.38477450609207153, accuracy: 0.9527777777777777\n",
      "Loss in epoch 9: 0.3311988413333893, accuracy: 0.9422222222222222\n",
      "Accuracy on test set: 0.8\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.3\n",
    "minibatch_size = 32\n",
    "for epoch in range(num_epochs):\n",
    "# minibatch training\n",
    "    epoch_loss = 0\n",
    "    num_minibatches = X_train.shape[0] // minibatch_size\n",
    "    for X_mini, y_mini in minibatches(X_train, y_train, minibatch_size):\n",
    "# we use a GradientTape to record the gradient for each minibatch\n",
    "        with tf.GradientTape() as t:\n",
    "            # important: we have to transpose here! each example is in a column\n",
    "            mini_loss = loss(forward_pass(X_mini.T), y_mini[None, :])\n",
    "        # update the weights\n",
    "        dW1, db1, dW15, db15, dW2, db2 = t.gradient(mini_loss, [W1, b1, W15, b15, W2, b2])\n",
    "        W1.assign_sub(learning_rate * dW1)\n",
    "        b1.assign_sub(learning_rate * db1)\n",
    "        W15.assign_sub(learning_rate * dW15)\n",
    "        b15.assign_sub(learning_rate * db15)\n",
    "        W2.assign_sub(learning_rate * dW2)\n",
    "        b2.assign_sub(learning_rate * db2)\n",
    "        epoch_loss += mini_loss\n",
    "    epoch_loss /= num_minibatches\n",
    "    predictions = forward_pass(X_train.T)\n",
    "    predictions = np.array([0 if pred < 0 else 1 for pred in tf.squeeze(predictions)])\n",
    "    accuracy = (y_train == predictions).sum() / len(y_train)\n",
    "    print(\"Loss in epoch {}: {}, accuracy: {}\".format(epoch, epoch_loss, accuracy))\n",
    "# accuracy on test set\n",
    "predictions = forward_pass(X_test.T)\n",
    "predictions = np.array([0 if pred < 0 else 1 for pred in tf.squeeze(predictions)])\n",
    "accuracy = (y_test == predictions).sum() / len(y_test)\n",
    "print(\"Accuracy on test set: {}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
