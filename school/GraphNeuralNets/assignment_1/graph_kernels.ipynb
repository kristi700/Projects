{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import signal\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import from_networkx, subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphlet Kernel Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphlet_features(data, k=3):\n",
    "    # NOTE - might need to add sampling instead of enumeration, to deal with snap data\n",
    "    features = {}\n",
    "    nodes = list(range(data.num_nodes))\n",
    "    for subset in itertools.combinations(nodes, k):\n",
    "        subset = list(subset)\n",
    "        sub_edge_index, _ = subgraph(subset, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "        if sub_edge_index.size(1) == 0:\n",
    "            continue\n",
    "        \n",
    "        sub_edge_index_np = sub_edge_index.cpu().numpy()\n",
    "        unique_edges = set()\n",
    "        for i in range(sub_edge_index_np.shape[1]):\n",
    "            u, v = sub_edge_index_np[0, i], sub_edge_index_np[1, i]\n",
    "            if u > v:\n",
    "                u, v = v, u\n",
    "            unique_edges.add((u, v))\n",
    "        \n",
    "        if len(unique_edges) < k - 1:\n",
    "            continue\n",
    "        \n",
    "        degrees = [0] * k\n",
    "        for u, v in unique_edges:\n",
    "            degrees[u] += 1\n",
    "            degrees[v] += 1\n",
    "        label = tuple(sorted(degrees))\n",
    "        features[label] = features.get(label, 0) + 1\n",
    "    return features\n",
    "\n",
    "def graphlet_kernel(data1, data2, k=3):\n",
    "\n",
    "    feat1 = graphlet_features(data1, k)\n",
    "    feat2 = graphlet_features(data2, k)\n",
    "    \n",
    "    keys = set(feat1.keys()) | set(feat2.keys())\n",
    "    \n",
    "    vec1 = torch.tensor([feat1.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    vec2 = torch.tensor([feat2.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    \n",
    "    kernel_value = torch.dot(vec1, vec2)\n",
    "    return kernel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphlet Kernel between BA and WS synthetic graphs: 294924.0\n"
     ]
    }
   ],
   "source": [
    "G_ba = nx.barabasi_albert_graph(n=50, m=4)\n",
    "G_ws = nx.watts_strogatz_graph(n=50, k=4, p=0.1)\n",
    "\n",
    "data_ba = from_networkx(G_ba)\n",
    "data_ws = from_networkx(G_ws)\n",
    "\n",
    "kernel_synthetic = graphlet_kernel(data_ba, data_ws, k=3)\n",
    "print(\"Graphlet Kernel between BA and WS synthetic graphs:\", kernel_synthetic.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wl_features(data, h=2):\n",
    "    if hasattr(data, 'x') and data.x is not None:\n",
    "        labels = [str(tuple(x.tolist())) for x in data.x]\n",
    "    else:\n",
    "        labels = ['1'] * data.num_nodes\n",
    "\n",
    "    feature_dict = defaultdict(int)\n",
    "    for lab in labels:\n",
    "        feature_dict[lab] += 1\n",
    "\n",
    "    label_lookup = {}\n",
    "    next_label_id = 0\n",
    "\n",
    "    def compress(label_str):\n",
    "        nonlocal next_label_id\n",
    "        if label_str not in label_lookup:\n",
    "            label_lookup[label_str] = str(next_label_id)\n",
    "            next_label_id += 1\n",
    "        return label_lookup[label_str]\n",
    "\n",
    "    for it in range(h):\n",
    "        new_labels = [None] * data.num_nodes\n",
    "        for v in range(data.num_nodes):\n",
    "            neighbors = data.edge_index[1, data.edge_index[0] == v].tolist() if data.edge_index.size(0) > 0 else []\n",
    "            multiset = sorted([labels[u] for u in neighbors])\n",
    "            new_label_str = labels[v] + \"_\" + \"_\".join(multiset)\n",
    "            new_labels[v] = compress(new_label_str)\n",
    "            feature_dict[new_labels[v]] += 1\n",
    "        labels = new_labels  \n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "def wl_kernel(data1, data2, h=2):\n",
    "    feat1 = wl_features(data1, h)\n",
    "    feat2 = wl_features(data2, h)\n",
    "    keys = set(feat1.keys()) | set(feat2.keys())\n",
    "    vec1 = torch.tensor([feat1.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    vec2 = torch.tensor([feat2.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    return torch.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded public graph with 4039 nodes and 88234 edges.\n",
      "Graphlet Kernel computation exceeded the maximum time limit.\n"
     ]
    }
   ],
   "source": [
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException()\n",
    "\n",
    "max_time = 300 # 5 mins\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "signal.alarm(max_time)\n",
    "try:\n",
    "    file_path = 'snap_dataset/facebook_combined.txt'\n",
    "    G_public = nx.read_edgelist(file_path, nodetype=int)\n",
    "    print(f\"Loaded public graph with {G_public.number_of_nodes()} nodes and {G_public.number_of_edges()} edges.\")\n",
    "    data_public = from_networkx(G_public)\n",
    "    kernel_value = graphlet_kernel(data_public, G_ba, k=3)\n",
    "    signal.alarm(0)\n",
    "    print(\"Graphlet Kernel (public graph vs. itself):\", kernel_value.item())\n",
    "except TimeoutException:\n",
    "    print(\"Graphlet Kernel computation exceeded the maximum time limit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WL Kernel (Graph 0 vs. Graph 1): 211.0\n",
      "WL Kernel Matrix shape: torch.Size([188, 188])\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='/tmp/TUDataset', name='MUTAG')\n",
    "\n",
    "data1 = dataset[0]\n",
    "data2 = dataset[1]\n",
    "\n",
    "kernel_val_wl = wl_kernel(data1, data2, h=2)\n",
    "print(\"WL Kernel (Graph 0 vs. Graph 1):\", kernel_val_wl.item())\n",
    "\n",
    "num_graphs = len(dataset)\n",
    "wl_kernel_matrix = torch.zeros((num_graphs, num_graphs))\n",
    "wl_features_list = [wl_features(data, h=2) for data in dataset]\n",
    "\n",
    "all_labels = set()\n",
    "for feat in wl_features_list:\n",
    "    all_labels.update(feat.keys())\n",
    "\n",
    "def features_to_vector(feat, keys):\n",
    "    return torch.tensor([feat.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "\n",
    "all_labels = list(all_labels) \n",
    "feature_vectors = [features_to_vector(feat, all_labels) for feat in wl_features_list]\n",
    "\n",
    "for i in range(num_graphs):\n",
    "    for j in range(i, num_graphs):\n",
    "        k_val = torch.dot(feature_vectors[i], feature_vectors[j])\n",
    "        wl_kernel_matrix[i, j] = k_val\n",
    "        wl_kernel_matrix[j, i] = k_val\n",
    "\n",
    "print(\"WL Kernel Matrix shape:\", wl_kernel_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphlet Kernel (Graph 0 vs. Graph 1): 513.0\n",
      "WL Kernel (Graph 0 vs. Graph 1): 211.0\n",
      "WL Kernel time: 0.000715 seconds\n",
      "Graphlet Kernel time: 0.029062 seconds\n"
     ]
    }
   ],
   "source": [
    "kernel_val_graphlet = graphlet_kernel(data1, data2, k=3)\n",
    "kernel_val_wl = wl_kernel(data1, data2, h=2)\n",
    "\n",
    "print(\"Graphlet Kernel (Graph 0 vs. Graph 1):\", kernel_val_graphlet.item())\n",
    "print(\"WL Kernel (Graph 0 vs. Graph 1):\", kernel_val_wl.item())\n",
    "\n",
    "start_time = time.time()\n",
    "_ = wl_kernel(data1, data2, h=2)\n",
    "wl_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "_ = graphlet_kernel(data1, data2, k=3)\n",
    "graphlet_time = time.time() - start_time\n",
    "\n",
    "print(f\"WL Kernel time: {wl_time:.6f} seconds\")\n",
    "print(f\"Graphlet Kernel time: {graphlet_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
