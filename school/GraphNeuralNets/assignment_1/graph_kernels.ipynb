{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.utils import from_networkx, subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphlet Kernel Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphlet_features(data, k=3):\n",
    "    # NOTE - might need to add sampling instead of enumeration, to deal with snap data\n",
    "    features = {}\n",
    "    nodes = list(range(data.num_nodes))\n",
    "    for subset in itertools.combinations(nodes, k):\n",
    "        subset = list(subset)\n",
    "        sub_edge_index, _ = subgraph(subset, data.edge_index, relabel_nodes=True, num_nodes=data.num_nodes)\n",
    "        if sub_edge_index.size(1) == 0:\n",
    "            continue\n",
    "        \n",
    "        sub_edge_index_np = sub_edge_index.cpu().numpy()\n",
    "        unique_edges = set()\n",
    "        for i in range(sub_edge_index_np.shape[1]):\n",
    "            u, v = sub_edge_index_np[0, i], sub_edge_index_np[1, i]\n",
    "            if u > v:\n",
    "                u, v = v, u\n",
    "            unique_edges.add((u, v))\n",
    "        \n",
    "        if len(unique_edges) < k - 1:\n",
    "            continue\n",
    "        \n",
    "        degrees = [0] * k\n",
    "        for u, v in unique_edges:\n",
    "            degrees[u] += 1\n",
    "            degrees[v] += 1\n",
    "        label = tuple(sorted(degrees))\n",
    "        features[label] = features.get(label, 0) + 1\n",
    "    return features\n",
    "\n",
    "def graphlet_kernel(data1, data2, k=3):\n",
    "\n",
    "    feat1 = graphlet_features(data1, k)\n",
    "    feat2 = graphlet_features(data2, k)\n",
    "    \n",
    "    keys = set(feat1.keys()) | set(feat2.keys())\n",
    "    \n",
    "    vec1 = torch.tensor([feat1.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    vec2 = torch.tensor([feat2.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    \n",
    "    kernel_value = torch.dot(vec1, vec2)\n",
    "    return kernel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphlet Kernel between BA and WS synthetic graphs: 289001.0\n"
     ]
    }
   ],
   "source": [
    "G_ba = nx.barabasi_albert_graph(n=50, m=4)\n",
    "G_ws = nx.watts_strogatz_graph(n=50, k=4, p=0.1)\n",
    "\n",
    "data_ba = from_networkx(G_ba)\n",
    "data_ws = from_networkx(G_ws)\n",
    "\n",
    "kernel_synthetic = graphlet_kernel(data_ba, data_ws, k=3)\n",
    "print(\"Graphlet Kernel between BA and WS synthetic graphs:\", kernel_synthetic.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wl_features(data, h=2):\n",
    "    if hasattr(data, 'x') and data.x is not None:\n",
    "        labels = [str(tuple(x.tolist())) for x in data.x]\n",
    "    else:\n",
    "        labels = ['1'] * data.num_nodes\n",
    "\n",
    "    feature_dict = defaultdict(int)\n",
    "    for lab in labels:\n",
    "        feature_dict[lab] += 1\n",
    "\n",
    "    label_lookup = {}\n",
    "    next_label_id = 0\n",
    "\n",
    "    def compress(label_str):\n",
    "        nonlocal next_label_id\n",
    "        if label_str not in label_lookup:\n",
    "            label_lookup[label_str] = str(next_label_id)\n",
    "            next_label_id += 1\n",
    "        return label_lookup[label_str]\n",
    "\n",
    "    for it in range(h):\n",
    "        new_labels = [None] * data.num_nodes\n",
    "        for v in range(data.num_nodes):\n",
    "            neighbors = data.edge_index[1, data.edge_index[0] == v].tolist() if data.edge_index.size(0) > 0 else []\n",
    "            multiset = sorted([labels[u] for u in neighbors])\n",
    "            new_label_str = labels[v] + \"_\" + \"_\".join(multiset)\n",
    "            new_labels[v] = compress(new_label_str)\n",
    "            feature_dict[new_labels[v]] += 1\n",
    "        labels = new_labels  \n",
    "\n",
    "    return feature_dict\n",
    "\n",
    "def wl_kernel(data1, data2, h=2):\n",
    "    feat1 = wl_features(data1, h)\n",
    "    feat2 = wl_features(data2, h)\n",
    "    keys = set(feat1.keys()) | set(feat2.keys())\n",
    "    vec1 = torch.tensor([feat1.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    vec2 = torch.tensor([feat2.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "    return torch.dot(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded public graph with 4039 nodes and 88234 edges.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded public graph with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG_public\u001b[38;5;241m.\u001b[39mnumber_of_nodes()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m nodes and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mG_public\u001b[38;5;241m.\u001b[39mnumber_of_edges()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m edges.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m data_public \u001b[38;5;241m=\u001b[39m from_networkx(G_public)\n\u001b[0;32m----> 6\u001b[0m kernel_value \u001b[38;5;241m=\u001b[39m \u001b[43mgraphlet_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_public\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_ba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphlet Kernel (public graph vs. itself):\u001b[39m\u001b[38;5;124m\"\u001b[39m, kernel_value\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mgraphlet_kernel\u001b[0;34m(data1, data2, k)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgraphlet_kernel\u001b[39m(data1, data2, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     feat1 \u001b[38;5;241m=\u001b[39m \u001b[43mgraphlet_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     feat2 \u001b[38;5;241m=\u001b[39m graphlet_features(data2, k)\n\u001b[1;32m     35\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(feat1\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(feat2\u001b[38;5;241m.\u001b[39mkeys())\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mgraphlet_features\u001b[0;34m(data, k)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mcombinations(nodes, k):\n\u001b[1;32m      6\u001b[0m     subset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(subset)\n\u001b[0;32m----> 7\u001b[0m     sub_edge_index, _ \u001b[38;5;241m=\u001b[39m \u001b[43msubgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelabel_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sub_edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_geometric/utils/_subgraph.py:145\u001b[0m, in \u001b[0;36msubgraph\u001b[0;34m(subset, edge_index, edge_attr, relabel_nodes, num_nodes, return_edge_mask)\u001b[0m\n\u001b[1;32m    143\u001b[0m edge_mask \u001b[38;5;241m=\u001b[39m node_mask[edge_index[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m&\u001b[39m node_mask[edge_index[\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    144\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, edge_mask]\n\u001b[0;32m--> 145\u001b[0m edge_attr \u001b[38;5;241m=\u001b[39m edge_attr[edge_mask] \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m relabel_nodes:\n\u001b[1;32m    148\u001b[0m     edge_index, _ \u001b[38;5;241m=\u001b[39m map_index(\n\u001b[1;32m    149\u001b[0m         edge_index\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    150\u001b[0m         subset,\n\u001b[1;32m    151\u001b[0m         max_index\u001b[38;5;241m=\u001b[39mnum_nodes,\n\u001b[1;32m    152\u001b[0m         inclusive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    153\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = 'snap_dataset/facebook_combined.txt'\n",
    "\n",
    "G_public = nx.read_edgelist(file_path, nodetype=int)\n",
    "print(f\"Loaded public graph with {G_public.number_of_nodes()} nodes and {G_public.number_of_edges()} edges.\")\n",
    "data_public = from_networkx(G_public)\n",
    "kernel_value = graphlet_kernel(data_public, G_ba, k=3)\n",
    "print(\"Graphlet Kernel (public graph vs. itself):\", kernel_value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WL Kernel (Graph 0 vs. Graph 1): 211.0\n",
      "WL Kernel Matrix shape: torch.Size([188, 188])\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='/tmp/TUDataset', name='MUTAG')\n",
    "\n",
    "data1 = dataset[0]\n",
    "data2 = dataset[1]\n",
    "\n",
    "kernel_val_wl = wl_kernel(data1, data2, h=2)\n",
    "print(\"WL Kernel (Graph 0 vs. Graph 1):\", kernel_val_wl.item())\n",
    "\n",
    "num_graphs = len(dataset)\n",
    "wl_kernel_matrix = torch.zeros((num_graphs, num_graphs))\n",
    "wl_features_list = [wl_features(data, h=2) for data in dataset]\n",
    "\n",
    "all_labels = set()\n",
    "for feat in wl_features_list:\n",
    "    all_labels.update(feat.keys())\n",
    "\n",
    "def features_to_vector(feat, keys):\n",
    "    return torch.tensor([feat.get(key, 0) for key in keys], dtype=torch.float32)\n",
    "\n",
    "all_labels = list(all_labels) \n",
    "feature_vectors = [features_to_vector(feat, all_labels) for feat in wl_features_list]\n",
    "\n",
    "for i in range(num_graphs):\n",
    "    for j in range(i, num_graphs):\n",
    "        k_val = torch.dot(feature_vectors[i], feature_vectors[j])\n",
    "        wl_kernel_matrix[i, j] = k_val\n",
    "        wl_kernel_matrix[j, i] = k_val\n",
    "\n",
    "print(\"WL Kernel Matrix shape:\", wl_kernel_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphlet Kernel (Graph 0 vs. Graph 1): 513.0\n",
      "WL Kernel (Graph 0 vs. Graph 1): 211.0\n",
      "WL Kernel time: 0.000824 seconds\n",
      "Graphlet Kernel time: 0.031891 seconds\n"
     ]
    }
   ],
   "source": [
    "kernel_val_graphlet = graphlet_kernel(data1, data2, k=3)\n",
    "kernel_val_wl = wl_kernel(data1, data2, h=2)\n",
    "\n",
    "print(\"Graphlet Kernel (Graph 0 vs. Graph 1):\", kernel_val_graphlet.item())\n",
    "print(\"WL Kernel (Graph 0 vs. Graph 1):\", kernel_val_wl.item())\n",
    "\n",
    "start_time = time.time()\n",
    "_ = wl_kernel(data1, data2, h=2)\n",
    "wl_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "_ = graphlet_kernel(data1, data2, k=3)\n",
    "graphlet_time = time.time() - start_time\n",
    "\n",
    "print(f\"WL Kernel time: {wl_time:.6f} seconds\")\n",
    "print(f\"Graphlet Kernel time: {graphlet_time:.6f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
