{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOADING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle datasets download -d devicharith/language-translation-englishfrench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip language-translation-englishfrench.zip -d language-translation-englishfrench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 09:50:01.986883: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 09:50:01.995421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-01 09:50:02.004960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-01 09:50:02.007902: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 09:50:02.015722: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733046604.288860    8520 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733046604.293356    8520 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733046604.293475    8520 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA EXPROLATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 175621\n",
      "Min words in english sentences: 1\n",
      "Min words in french sentences: 1\n",
      "Max words in english sentences: 44\n",
      "Max words in french sentences: 55\n",
      "Average words in english sentences: 6.161552433934438\n",
      "Average words in french sentences: 6.706669475746067\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English words/sentences</th>\n",
       "      <th>French words/sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Salut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who?</td>\n",
       "      <td>Qui ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  English words/sentences French words/sentences\n",
       "0                     Hi.                 Salut!\n",
       "1                    Run!                Cours !\n",
       "2                    Run!               Courez !\n",
       "3                    Who?                  Qui ?\n",
       "4                    Wow!             Ça alors !"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_french_data = pd.read_csv(\n",
    "    \"language-translation-englishfrench/eng_-french.csv\", encoding=\"utf-8\"\n",
    ")\n",
    "print(f\"Number of sentences: {len(english_french_data)}\")\n",
    "english_sentence_words = english_french_data[\"English words/sentences\"].apply(\n",
    "    lambda x: len(str(x).split())\n",
    ")\n",
    "french_avg_sentence_words = english_french_data[\"French words/sentences\"].apply(\n",
    "    lambda x: len(str(x).split())\n",
    ")\n",
    "print(f\"Min words in english sentences: {english_sentence_words.min()}\")\n",
    "print(f\"Min words in french sentences: {french_avg_sentence_words.min()}\")\n",
    "print(f\"Max words in english sentences: {english_sentence_words.max()}\")\n",
    "print(f\"Max words in french sentences: {french_avg_sentence_words.max()}\")\n",
    "print(f\"Average words in english sentences: {english_sentence_words.mean()}\")\n",
    "print(f\"Average words in french sentences: {french_avg_sentence_words.mean()}\")\n",
    "english_french_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175621 entries, 0 to 175620\n",
      "Data columns (total 2 columns):\n",
      " #   Column                   Non-Null Count   Dtype \n",
      "---  ------                   --------------   ----- \n",
      " 0   English words/sentences  175621 non-null  object\n",
      " 1   French words/sentences   175621 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "english_french_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the dataset as it was grouped from shortest to longest\n",
    "english_french_data = english_french_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_en = english_french_data[\"English words/sentences\"].to_numpy()\n",
    "sentences_fr = english_french_data[\"French words/sentences\"].to_numpy()\n",
    "val_len = int(0.1 * len(english_french_data))\n",
    "\n",
    "sentences_en_train = sentences_en[:-val_len]\n",
    "sentences_fr_train = sentences_fr[:-val_len]\n",
    "\n",
    "sentences_en_val = sentences_en[-val_len:]\n",
    "sentences_fr_val = sentences_fr[-val_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_and_target(sentences_en, sentences_fr):\n",
    "    return (sentences_en, b\"<SOS> \" + sentences_fr), sentences_fr + b\" <EOS>\"\n",
    "\n",
    "\n",
    "def from_sentences_dataset(\n",
    "    sentences_en,\n",
    "    sentences_fr,\n",
    "    batch_size,\n",
    "):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((sentences_en, sentences_fr))\n",
    "    dataset = dataset.map(prepare_input_and_target, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = from_sentences_dataset(sentences_en_train, sentences_fr_train, batch_size=128)\n",
    "val_dataset = from_sentences_dataset(sentences_en_val, sentences_fr_val,  batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "):\n",
    "    model.vectorization_en.adapt(\n",
    "        train_dataset.map(\n",
    "            lambda sentences, target: sentences[0]\n",
    "        )\n",
    "    )\n",
    "    model.vectorization_fr.adapt(\n",
    "        train_dataset.map(\n",
    "            lambda sentences, target: sentences[1] + b\" <EOS>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_dataset= train_dataset.map(\n",
    "        lambda sentences, target: (sentences, model.vectorization_fr(target))\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    valid_dataset= valid_dataset.map(\n",
    "        lambda sentences, target: (sentences, model.vectorization_fr(target))\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpu doesnt work for tf in this docker, if training is rlly slow, look into it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, max_length, embedding_dim):\n",
    "        super().__init__()\n",
    "        if embedding_dim % 2 != 0:\n",
    "            raise ValueError(\"Embedding dimension must be an even number\")\n",
    "\n",
    "        positions = np.arange(max_length)\n",
    "        dimensions = np.arange(embedding_dim // 2)\n",
    "        angles = positions[:, np.newaxis] / (10000 ** (2 * dimensions / embedding_dim))\n",
    "        pos_encoding = np.zeros((max_length, embedding_dim))\n",
    "        pos_encoding[:, 0::2] = np.sin(angles)\n",
    "        pos_encoding[:, 1::2] = np.cos(angles)\n",
    "        self.positional_encoding = tf.constant(pos_encoding[np.newaxis, :, :], dtype=tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sequence_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.positional_encoding[:, :sequence_length, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        attention_heads,\n",
    "        dense_dim,\n",
    "        dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(\n",
    "            attention_heads, embed_size, dropout=dropout_rate\n",
    "        )\n",
    "        self.feed_forward = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_size),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.add = layers.Add()\n",
    "        self.normalization = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        skip_x= x\n",
    "        x = self.multi_head_attention(x, value=x, attention_mask=mask)\n",
    "        x = self.normalization(self.add([x, skip_x]))\n",
    "        skip_x = x\n",
    "        x = self.feed_forward(x)\n",
    "        return self.normalization(self.add([x, skip_x]))\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        attention_heads,\n",
    "        dense_dim,\n",
    "        dropout_rate,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.masked_multi_head_attention = layers.MultiHeadAttention(\n",
    "            attention_heads, embed_size, dropout=dropout_rate\n",
    "        )\n",
    "        self.cross_attention = layers.MultiHeadAttention(\n",
    "            attention_heads, embed_size, dropout=dropout_rate\n",
    "        )\n",
    "        self.feed_forward = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(\n",
    "                    dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_size),\n",
    "                layers.Dropout(dropout_rate),\n",
    "            ]\n",
    "        )\n",
    "        self.add = layers.Add()\n",
    "        self.normalization = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        decoder_mask, encoder_mask = mask\n",
    "        x, encoder_output = inputs\n",
    "        x_skip = x\n",
    "        x = self.masked_multi_head_attention(x, value=x, attention_mask=decoder_mask)\n",
    "        x = self.normalization(self.add([x, x_skip]))\n",
    "        x_skip = x\n",
    "        x = self.cross_attention(\n",
    "            x, value=encoder_output, attention_mask=encoder_mask\n",
    "        )\n",
    "        x = self.normalization(self.add([x, x_skip]))\n",
    "        x_skip = x\n",
    "        x = self.feed_forward(x)\n",
    "        return self.normalization(self.add([x, x_skip]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=5000,\n",
    "        max_seq_len=50,\n",
    "        embed_size=256,\n",
    "        encoder_decoder=1,\n",
    "        attention_heads=8,\n",
    "        dense_dim=256,\n",
    "        dropout_rate=0.2,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.vectorization_en = layers.TextVectorization(\n",
    "            vocab_size, output_sequence_length=max_seq_len\n",
    "        )\n",
    "        self.vectorization_fr = layers.TextVectorization(\n",
    "            vocab_size, output_sequence_length=max_seq_len\n",
    "        )\n",
    "        self.encoder_embedding = layers.Embedding(\n",
    "            vocab_size, embed_size, mask_zero=True\n",
    "        )\n",
    "        self.decoder_embedding = layers.Embedding(\n",
    "            vocab_size, embed_size, mask_zero=True\n",
    "        )\n",
    "        self.positional_encoding = PositionalEncoding(max_seq_len, embed_size)\n",
    "        self.encoder_blocks = [\n",
    "            Encoder(embed_size, attention_heads, dense_dim, dropout_rate)\n",
    "            for _ in range(encoder_decoder)\n",
    "        ]\n",
    "        self.decoder_blocks = [\n",
    "            Decoder(embed_size, attention_heads, dense_dim, dropout_rate)\n",
    "            for _ in range(encoder_decoder)\n",
    "        ]\n",
    "        self.output_layer = layers.Dense(vocab_size, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "\n",
    "        encoder_input_ids = self.vectorization_en(encoder_inputs)\n",
    "        decoder_input_ids = self.vectorization_fr(decoder_inputs)\n",
    "        encoder_embeddings = self.encoder_embedding(encoder_input_ids)\n",
    "        decoder_embeddings = self.decoder_embedding(decoder_input_ids)\n",
    "        encoder_pos_embeddings = self.positional_encoding(encoder_embeddings)\n",
    "        decoder_pos_embeddings = self.positional_encoding(decoder_embeddings)\n",
    "\n",
    "        encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "        decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "        batch_max_len_decoder = tf.shape(decoder_embeddings)[1]\n",
    "        decoder_causal_mask = tf.linalg.band_part(\n",
    "            tf.ones((batch_max_len_decoder, batch_max_len_decoder), tf.bool), -1, 0\n",
    "        )\n",
    "        decoder_mask = decoder_causal_mask & decoder_pad_mask\n",
    "\n",
    "        x = encoder_pos_embeddings\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x, mask=encoder_pad_mask)\n",
    "\n",
    "        encoder_output = x\n",
    "        x = decoder_pos_embeddings\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            x = decoder_block(\n",
    "                [x, encoder_output], mask=[decoder_mask, encoder_pad_mask]\n",
    "            )\n",
    "\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "train_dataset, val_dataset = prepare_dataset(model, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'positional_encoding_2' (of type PositionalEncoding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 41ms/step - accuracy: 0.8623 - loss: 1.1362 - val_accuracy: 0.8938 - val_loss: 0.6022\n",
      "Epoch 2/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 41ms/step - accuracy: 0.8952 - loss: 0.5893 - val_accuracy: 0.8991 - val_loss: 0.5572\n",
      "Epoch 3/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 40ms/step - accuracy: 0.8991 - loss: 0.5527 - val_accuracy: 0.9001 - val_loss: 0.5454\n",
      "Epoch 4/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 41ms/step - accuracy: 0.9000 - loss: 0.5424 - val_accuracy: 0.9003 - val_loss: 0.5420\n",
      "Epoch 5/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 40ms/step - accuracy: 0.9003 - loss: 0.5393 - val_accuracy: 0.9004 - val_loss: 0.5411\n",
      "Epoch 6/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 41ms/step - accuracy: 0.9004 - loss: 0.5384 - val_accuracy: 0.9004 - val_loss: 0.5408\n",
      "Epoch 7/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 38ms/step - accuracy: 0.9004 - loss: 0.5381 - val_accuracy: 0.9004 - val_loss: 0.5407\n",
      "Epoch 8/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 38ms/step - accuracy: 0.9004 - loss: 0.5380 - val_accuracy: 0.9004 - val_loss: 0.5407\n",
      "Epoch 9/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 38ms/step - accuracy: 0.9004 - loss: 0.5380 - val_accuracy: 0.9004 - val_loss: 0.5407\n",
      "Epoch 10/10\n",
      "\u001b[1m1235/1235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 38ms/step - accuracy: 0.9004 - loss: 0.5380 - val_accuracy: 0.9004 - val_loss: 0.5407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x724808df5b90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduled_lr = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-3, decay_steps=100, decay_rate=0.9)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=scheduled_lr),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
